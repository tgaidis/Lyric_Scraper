{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Variables\n",
    "alphabet = ['a','b','c','d','e','f','g','h','i','j',\n",
    "            'k','l','m','n','o','p','q','r','s','t',\n",
    "            'u','v','w','x','y','z']\n",
    "\n",
    "#AZLyrics Link\n",
    "az_link = \"https://www.azlyrics.com/\"\n",
    "\n",
    "#list for urls\n",
    "letter_urls = []\n",
    "\n",
    "#Generate urls for alphabet\n",
    "for letter in alphabet:\n",
    "    letter_urls.append(az_link + letter + \".html\")\n",
    "\n",
    "#Stuff\n",
    "artist_urls = []\n",
    "artist_names = []\n",
    "\n",
    "#Generate lists of all artists and their urls\n",
    "for url in letter_urls[5:6]:\n",
    "    #Collect artist names from page\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "    content = soup.find_all(class_=\"col-sm-6 text-center artist-col\")\n",
    "\n",
    "    #Fill link list and artist_names list\n",
    "    for col in content:\n",
    "        g_tags = col.find_all(\"a\")\n",
    "        for item in g_tags:\n",
    "            artist_urls.append(az_link + item.get('href'))\n",
    "            artist_names.append(item.text)\n",
    "    #Sleep to avoid being blocked\n",
    "    time.sleep(random.uniform(3,8))\n",
    "            \n",
    "#print(links, artist_names)\n",
    "artist_pages=list(zip(artist_names, artist_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize df and name columns\n",
    "with open('songInfo.csv', 'w', encoding=\"utf-8\", newline='') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow((\"Artist\", \"Album\", \"Year\", \"Song Title\", \"Lyrics\"))\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Song information collection \n",
    "for item in artist_pages[:100]:\n",
    "    artist_name = item[0]\n",
    "    url = item[1]\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "    divs = soup.find_all(\"div\")\n",
    "    current_album=\"\"\n",
    "    for div in divs:\n",
    "        if div.has_attr(\"class\"):\n",
    "            if div.attrs[\"class\"][0]==\"album\":\n",
    "                current_album = div.text\n",
    "            elif div.attrs[\"class\"][0] == \"listalbum-item\":\n",
    "                endLink = div.a.get(\"href\")\n",
    "                songURL = \"https://www.azlyrics.com/\" + endLink[3:]\n",
    "                curr_song = div.text\n",
    "                try:\n",
    "                    page1 = requests.get(songURL)\n",
    "                    soupp = BeautifulSoup(page1.content, 'lxml')\n",
    "                    x = soupp.find('div', {'class': 'col-xs-12 col-lg-8 text-center'})\n",
    "                    y = x.find_all('div')\n",
    "                    curr_year = current_album[-5:-1]\n",
    "                    lyrics = y[5].text\n",
    "\n",
    "                    try:\n",
    "                        year = int(curr_year)\n",
    "                        yearUP = str(year)\n",
    "                        Row = []\n",
    "                        Row.append(artist_name)\n",
    "                        Row.append(current_album)\n",
    "                        Row.append(yearUP)\n",
    "                        Row.append(curr_song)\n",
    "                        Row.append(artist_name)\n",
    "                        Row.append(lyrics)\n",
    "                        allData.append(Row)\n",
    "                        print(Row[:4])\n",
    "                        \n",
    "                        with open('songInfo.csv', 'a', encoding=\"utf-8\", newline='') as myfile:\n",
    "                            wr = csv.writer(myfile)\n",
    "                            wr.writerow(Row)\n",
    "                        myfile.close()\n",
    "\n",
    "                    except:\n",
    "                        print('Invalid Year, Ignoring Song')\n",
    "                except:\n",
    "                    print('Could Not Collect Page Data - Probably IP Blocked')\n",
    "                    \n",
    "                time.sleep(random.uniform(4,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
